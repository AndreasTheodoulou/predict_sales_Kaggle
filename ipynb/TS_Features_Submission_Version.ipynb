{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TS_Features_Submission_Version.ipynb","provenance":[{"file_id":"1nzPRIdf4UB-3biwx8fCJlTnx8bL126aO","timestamp":1588242465890},{"file_id":"https://github.com/migai/Kag/blob/master/template_Kaggle_Coursera_Final_Assignment.ipynb","timestamp":1587141076973}],"collapsed_sections":["YPp_Nesy2yxn","YQ7tffMSSfn7","i7KKi2LH7ZVU"],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YPp_Nesy2yxn"},"source":["#**Final Project for Coursera's 'How to Win a Data Science Competition'**\n","April, 2020\n","\n","\n","(Competition Info last updated:  3 years ago)"]},{"cell_type":"markdown","metadata":{"id":"r_Oe76PW3aoN"},"source":["##**About this Competition**\n","\n","You are provided with daily historical sales data. The task is to forecast the total amount of products sold in every shop for the test set. Note that the list of shops and products slightly changes every month. Creating a robust model that can handle such situations is part of the challenge.\n","\n","Evaluation: root mean squared error (RMSE). True target values are clipped into [0,20] range.\n","\n",".\n","\n","##**File descriptions**\n","\n","***sales_train.csv*** - the training set. Daily historical data from January 2013 to October 2015.\n","\n","***test.csv*** - the test set. You need to forecast the sales for these shops and products for November 2015.\n","\n","***sample_submission.csv*** - a sample submission file in the correct format.\n","\n","***items.csv*** - supplemental information about the items/products.\n","\n","***item_categories.csv***  - supplemental information about the items categories.\n","\n","***shops.csv***- supplemental information about the shops.\n","\n",".\n","\n","##**Data fields**\n","\n","***ID*** - an Id that represents a (Shop, Item) tuple within the test set\n","\n","***shop_id*** - unique identifier of a shop\n","\n","***item_id*** - unique identifier of a product\n","\n","***item_category_id*** - unique identifier of item category\n","\n","***item_cnt_day*** - number of products sold. You are predicting a monthly amount of this measure\n","\n","***item_price*** - current price of an item\n","\n","***date*** - date in format dd/mm/yyyy\n","\n","***date_block_num*** - a consecutive month number. January 2013 is 0, February 2013 is 1,..., October 2015 is 33\n","\n","***item_name*** - name of item\n","\n","***shop_name*** - name of shop\n","\n","***item_category_name*** - name of item category"]},{"cell_type":"markdown","metadata":{"id":"LyLQLqBcOnLt"},"source":["#Load Files\n","Load competition data files and import helpful custom code libraries from shared GitHub repository"]},{"cell_type":"code","metadata":{"id":"HweIOGsaAhvC","executionInfo":{"status":"ok","timestamp":1602333121292,"user_tz":-60,"elapsed":2053,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["#Load libraries\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from itertools import product\n","import time\n","import pickle\n","from lightgbm import LGBMRegressor\n","import lightgbm as lgb\n","import sklearn\n","import time"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"j0f9zzlTGqAt","executionInfo":{"status":"error","timestamp":1602333273281,"user_tz":-60,"elapsed":153985,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}},"outputId":"80951137-82f5-4527-b427-ed6da887abdf","colab":{"base_uri":"https://localhost:8080/","height":618}},"source":["#only use the below code (drive.mount()) if using google collab and files are stored on a google drive directory\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","file_dir_path = '/content/drive/My Drive/Colab Notebooks/Coursera_Data_Science_Competitions_Kaggle_project/Kag'"],"execution_count":2,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \"\"\"\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-bcc0ef874dec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#only use the below code (drive.mount()) if using google collab and files are stored on a google drive directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    249\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dfs-auth-dance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m           \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"uSth8n6q_qvb","executionInfo":{"status":"aborted","timestamp":1602333273246,"user_tz":-60,"elapsed":153942,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["#Basic data files (provided by kaggle)\n","data_folder =  '/data_output'\n","items = pd.read_csv(file_dir_path + data_folder +  '/items.csv')\n","sales_train = pd.read_csv(file_dir_path + data_folder + '/sales_train.csv.gz')\n","test = pd.read_csv(file_dir_path + data_folder + '/test.csv.gz')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kGlAHKpR8098","executionInfo":{"status":"aborted","timestamp":1602333273249,"user_tz":-60,"elapsed":153936,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["#Created data files\n","days_by_month = pd.read_csv(file_dir_path +  data_folder + '/days_by_month.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MEB4_s4KJ0qz","executionInfo":{"status":"aborted","timestamp":1602333273251,"user_tz":-60,"elapsed":153925,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["#Code Controls - keep as set\n","load_model = True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QLd06tZbokLb"},"source":["Helpful functions"]},{"cell_type":"code","metadata":{"id":"qTGQ-IzUojgP","executionInfo":{"status":"aborted","timestamp":1602333273253,"user_tz":-60,"elapsed":153920,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["def lag_feature(df, lags, col):\n","    tmp = df[['date_block_num','shop_id','item_id',col]]\n","    for i in range(len(lags)):\n","      shifted_df = tmp.copy()\n","      shifted_df.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(lags[i])]\n","      shifted_df['date_block_num'] += lags[i]   \n","      if i == 0:\n","        shifted_df_merged = shifted_df\n","      else:\n","        shifted_df_merged = pd.merge(shifted_df_merged, shifted_df, on=['date_block_num','shop_id','item_id'], how='left')\n","    #shifted_df_merged  = downcast_df(shifted_df_merged)\n","    return shifted_df_merged\n","\n","import numpy as np\n","def downcast_df(data):\n","  cols_not_to_downcast = ['item_id', 'ID','item_cnt_month', 'sales_sum_by_month', 'cluster_code', 'price_']\n","  for col in data.columns:\n","    col_type = data[col].dtype\n","    if np.issubdtype(col_type, np.integer):\n","      if col in cols_not_to_downcast:\n","        data[col] = data[col].astype(np.int32)\n","      else:    \n","        data[col] = data[col].astype(np.int16)\n","    elif np.issubdtype(col_type, np.floating):\n","      if col in cols_not_to_downcast:\n","        data[col] = data[col].astype(np.float32) \n","      else:    \n","        data[col] = data[col].astype(np.float16) #if below 32 df.describe() mean and std are NaN. but no difference in results for float16 vs float32 (so can use float16 if needed and memory crashes)\n","    else:\n","      pass\n","  return data\n","\n","def upcast_df(data):\n","  cols_not_to_downcast = ['item_id', 'ID']\n","  for col in data.columns:\n","    col_type = data[col].dtype\n","    if np.issubdtype(col_type, np.integer):\n","      if col in cols_not_to_downcast:\n","        data[col] = data[col].astype(np.int32)\n","      else:    \n","        data[col] = data[col].astype(np.int32)\n","    elif np.issubdtype(col_type, np.floating):\n","      data[col] = data[col].astype(np.float64)\n","    else:\n","      pass\n","  return data\n","\n","\n","def infer_variable_types(data):\n","  variable_types = {'categorical': [], 'numerical': []}\n","  for col in data.columns:\n","    col_type = data[col].dtype\n","    if np.issubdtype(col_type, np.integer):\n","      variable_types['categorical'].append(col)\n","    elif np.issubdtype(col_type, np.floating):\n","      variable_types['numerical'].append(col)\n","    else:\n","      pass\n","  return variable_types \n","\n","def sort_variable_types(data, categorical_cols, numerical_cols):\n","  cols_not_to_downcast = ['item_id', 'ID']\n","  for col in data.columns:\n","    if col in categorical_cols:\n","      if col in cols_not_to_downcast:\n","        data[col] = data[col].astype(np.int32)\n","      else:    \n","        data[col] = data[col].astype(np.int8)\n","    elif col in numerical_cols:\n","      data[col] = data[col].astype(np.float16)\n","    else:\n","      pass\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YQ7tffMSSfn7"},"source":["# Data Preparation\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MXYTT_-iOOZY"},"source":["Make monthly table (\"matrix\") of Shop-Item pairs (using cartesian product of date_block_num, shop_id, item_id)"]},{"cell_type":"code","metadata":{"id":"2uRVAWaX0qqJ","executionInfo":{"status":"aborted","timestamp":1602333273254,"user_tz":-60,"elapsed":153900,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["matrix = []\n","cols = ['date_block_num','shop_id','item_id']\n","for i in range(34):\n","    sales = sales_train[sales_train.date_block_num==i]\n","    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))    \n","matrix = pd.DataFrame(np.vstack(matrix), columns=cols)\n","\n","matrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\n","matrix['shop_id'] = matrix['shop_id'].astype(np.int8)\n","matrix['item_id'] = matrix['item_id'].astype(np.int16)\n","matrix.sort_values(cols,inplace=True)\n","print(\"monthly table is\")\n","matrix.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P_2BcZ7oznV5"},"source":["Include test set  - and its test IDs (of shop-item pairs) given from test.csv[link text"]},{"cell_type":"code","metadata":{"id":"b6rT92bz0pXm","executionInfo":{"status":"aborted","timestamp":1602333273255,"user_tz":-60,"elapsed":153888,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["test['date_block_num'] = 34\n","test['date_block_num'] = test['date_block_num'].astype(np.int8)\n","test['shop_id'] = test['shop_id'].astype(np.int8)\n","test['item_id'] = test['item_id'].astype(np.int16)\n","\n","matrix = pd.concat([matrix, test], ignore_index=True, sort=False, keys=cols)\n","matrix.fillna(0, inplace=True) # 34 month\n","matrix['ID'] = matrix['ID'].astype(np.int32)\n","\n","matrix"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S07JtQZmz3gX"},"source":["Add monthly item_count (sales) to the monthly table"]},{"cell_type":"code","metadata":{"id":"mCoe7RDJ0vbm","executionInfo":{"status":"aborted","timestamp":1602333273256,"user_tz":-60,"elapsed":153877,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["ts = time.time()\n","monthly_sales = sales_train.groupby(['date_block_num', 'shop_id', 'item_id']).agg({'item_cnt_day': ['sum']})\n","monthly_sales.columns = ['item_cnt_month']\n","monthly_sales.reset_index(inplace=True)\n","\n","matrix = pd.merge(matrix, monthly_sales, on=cols, how='left')\n","matrix['item_cnt_month'] = (matrix['item_cnt_month']\n","                                .fillna(0)\n","                                .clip(0,20) # NB clip target here\n","                                .astype(np.float32))\n","matrix.tail()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CNW58WF_HsfU","executionInfo":{"status":"aborted","timestamp":1602333273257,"user_tz":-60,"elapsed":153870,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["#matrix to merge after groupbys\n","matrix = pd.merge(matrix, items[['item_id', 'item_category_id']], on=['item_id'], how='left')\n","#sales_train to groupby later\n","sales_train = pd.merge(sales_train, items[['item_id', 'item_category_id']], on=['item_id'], how='left')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a4ll1tYJLCud","executionInfo":{"status":"aborted","timestamp":1602333273259,"user_tz":-60,"elapsed":153861,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["#Replace NaNs with 0\n","matrix.fillna(0, inplace=True)\n","\n","#Downcast variables (smaller memory size)\n","matrix = downcast_df(matrix)\n","matrix.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n0fhfEVW2Pm-"},"source":["Understanding dataframe created (\"matrix\")\n","\n"]},{"cell_type":"code","metadata":{"id":"QBP59nqnt43y","executionInfo":{"status":"aborted","timestamp":1602333273259,"user_tz":-60,"elapsed":153852,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["#NOTE: For item_category_features we have a very small percentage of null values (maybe a few item ids present in matrix, not mapped to an item category during analysis)\n","df = matrix\n","df1 = df.describe(include = 'all')\n","\n","df1.loc['dtype'] = df.dtypes\n","df1.loc['size'] = len(df)\n","df1.loc['% null count'] = df.isnull().mean()\n","df1.loc['count of 0s'] = df.apply(lambda col: (col.count() - np.count_nonzero(col)))\n","df1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VgzkOq08BH3l"},"source":["### **Featue Generation/Engineering**\n","\n","Time series features\n","*   Statistics of previous months (e.g. mean of item_price for a specific item/shop in previous months)\n","*   Trends of previous months - rate of change of the above statistics based features (e.g. rate of change of mean item_price from today to the past 3 months for a specific shop/item)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IZzHeIovUrS5"},"source":["Stage 1: Statistics based features\n","\n","> 1st step: Compute their Values\n"]},{"cell_type":"code","metadata":{"id":"B9cEnEK1UduX","executionInfo":{"status":"aborted","timestamp":1602333273260,"user_tz":-60,"elapsed":153844,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["#----------------------------------------Features of TS&Stats v1 (v1.4) --------------------------------------------------\n","#Compute sum, mean, std, and median of item price/item cnt for the below categories:\n","#include count of items sold in a shop (size of shop), count of days in the month)\n","\n","\n","#1)\n","#per date_block_num and item_id\n","monthly_sales_per_item = sales_train.groupby(['date_block_num','item_id']).agg({'item_price': ['mean', 'std', 'median'],\n","                                                              'item_cnt_day': ['sum', 'mean', 'std', 'median']\n","                                                              })\n","monthly_sales_per_item.columns = ['price_mean_by_item', 'price_std_by_item', 'price_median_by_item',\n","                'sales_sum_by_item', 'sales_mean_by_item', 'sales_std_by_item', 'sales_median_by_item']\n","monthly_sales_per_item.reset_index(inplace=True)\n","matrix = pd.merge(matrix, monthly_sales_per_item, on=['date_block_num','item_id'], how='left')\n","\n","#2)\n","#per date_block_num and shop_id\n","monthly_sales_per_shop = sales_train.groupby(['date_block_num','shop_id']).agg({'item_price': ['mean', 'std', 'median'],\n","                                                              'item_cnt_day': ['sum', 'mean', 'std', 'median']})\n","monthly_sales_per_shop.columns = ['price_mean_by_shop', 'price_std_by_shop', 'price_median_by_shop',\n","                'sales_sum_by_shop', 'sales_mean_by_shop', 'sales_std_by_shop', 'sales_median_by_shop']\n","monthly_sales_per_shop.reset_index(inplace=True)\n","matrix = pd.merge(matrix, monthly_sales_per_shop, on=['date_block_num','shop_id'], how='left')\n","\n","#3)\n","#per date_block_num and item_category\n","monthly_sales_per_item_category = sales_train.groupby(['date_block_num','item_category_id']).agg({'item_price': ['mean', 'std', 'median'],\n","                                                              'item_cnt_day': ['sum', 'mean', 'std', 'median']})\n","monthly_sales_per_item_category.columns = ['price_mean_by_item_category', 'price_std_by_item_category', 'price_median_by_item_category',\n","                'sales_sum_by_item_category', 'sales_mean_by_item_category', 'sales_std_by_item_category', 'sales_median_by_item_category']\n","\n","monthly_sales_per_item_category.reset_index(inplace=True)\n","matrix = pd.merge(matrix, monthly_sales_per_item_category, on=['date_block_num','item_category_id'], how='left')\n","\n","#4)\n","#per date_block_num, item_id, and shop_id\n","monthly_sales_per_shop_and_item = sales_train.groupby(['date_block_num','item_id', 'shop_id']).agg({'item_price': ['mean', 'std', 'median'],\n","                                                              'item_cnt_day': ['sum', 'mean', 'std', 'median']})\n","monthly_sales_per_shop_and_item.columns = ['price_mean_by_item_and_shop', 'price_std_by_item_and_shop', 'price_median_by_item_and_shop',\n","                'sales_sum_by_item_and_shop', 'sales_mean_by_item_and_shop', 'sales_std_by_item_and_shop', 'sales_median_by_item_and_shop']\n","\n","monthly_sales_per_shop_and_item.reset_index(inplace=True)\n","matrix = pd.merge(matrix, monthly_sales_per_shop_and_item, on=['date_block_num','item_id', 'shop_id'], how='left')\n","\n","#5)\n","#per date_block_num\n","monthly_sales_per_date_block = sales_train.groupby(['date_block_num']).agg({'item_price': ['mean', 'std', 'median'],\n","                                                              'item_cnt_day': ['sum', 'mean', 'std', 'median']})\n","monthly_sales_per_date_block.columns = ['price_mean_by_month', 'price_std_by_month', 'price_median_by_month',\n","                'sales_sum_by_month', 'sales_mean_by_month', 'sales_std_by_month', 'sales_median_by_month']\n","\n","monthly_sales_per_date_block.reset_index(inplace=True)\n","matrix = pd.merge(matrix, monthly_sales_per_date_block, on=['date_block_num'], how='left')\n","\n","matrix.fillna(0, inplace=True) # 34 month\n","matrix = downcast_df(matrix)\n","\n","matrix\n","\n","TS_lags = [\n","          1, 3, 6, 12,\n","          2,           #Need 2,4,7,13 lags to calculate 1m, 3m, 6m, 12m trends \n","          #4, 7, 13\n","          ]\n","\n","TS_features = ['price_mean_by_item', 'sales_sum_by_item', \n","              'price_mean_by_shop', 'sales_sum_by_shop',\n","              'price_mean_by_item_category', 'sales_sum_by_item_category',\n","              'price_mean_by_item_and_shop', 'sales_sum_by_item_and_shop',\n","              'price_mean_by_month', 'sales_sum_by_month']\n","\n","Stats_lags = [1]\n","\n","Stats_features = [\n","                  'price_std_by_item', 'price_median_by_item', 'sales_std_by_item', 'sales_median_by_item', 'sales_mean_by_item',\n","                  'price_std_by_shop', 'price_median_by_shop', 'sales_std_by_shop', 'sales_median_by_shop', 'sales_mean_by_shop',\n","                  'price_std_by_item_category', 'price_median_by_item_category', 'sales_std_by_item_category', 'sales_median_by_item_category', 'sales_mean_by_item_category',\n","                  'price_std_by_item_and_shop', 'price_median_by_item_and_shop',  'sales_std_by_item_and_shop', 'sales_median_by_item_and_shop', 'sales_mean_by_item_and_shop',\n","                  'price_std_by_month', 'price_median_by_month', 'sales_std_by_month', 'sales_median_by_month',  'sales_mean_by_month'\n","                ]\n","trend_lags = [2, \n","              #4, 7, 13\n","              ]\n","\n","matrix.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"621H7P3rVTNZ"},"source":["> 2nd step: Lag them (put them in the same row/month as the one you'll be using them to predict - e.g e.g if going to use 6month ago mean of item_price to predict item_cnt of next month, put 6 month ago mean of item_price in the same row as current month's values, used to predict next month)\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"0M4DkX8SawXs","executionInfo":{"status":"aborted","timestamp":1602333273261,"user_tz":-60,"elapsed":153837,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["ts = time.time()\n","\n","#---------------------------Create Time series based features--------------------------\n","#TS based features = features computed based on stats (just mean in this case) of item price/cnt of shops or item at different previous months/lags\n","\n","#ToDo: parallelize this process and the below\n","\n","for i in range(len(TS_features)):\n","  matrix_lagged = lag_feature(matrix, TS_lags, TS_features[i])\n","  matrix = pd.merge(matrix, matrix_lagged, on=['date_block_num','shop_id','item_id'], how='left')\n","  matrix = downcast_df(matrix) \n","del matrix_lagged\n","print(time.time()-ts)\n","matrix.tail()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nFmSE7O4XU8r","executionInfo":{"status":"aborted","timestamp":1602333273262,"user_tz":-60,"elapsed":153828,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["#---------------------------Create Stats based features--------------------------------\n","#Stats based features = features computed based on stats of item price/cnt of shops or item for just the previous month\n","                                  \n","#Splitting Stats_features as below helped as a quick fix for the session getting crashed from running out of RAM (if GPU still crashes, use TPU for this one - has more RAM)\n","length = len(Stats_features)\n","index = length//3\n","Stats_features_first = Stats_features[:index]\n","Stats_features_second = Stats_features[index:(index*2)]\n","Stats_features_third = Stats_features[(index*2):]\n","\n","for i in range(len(Stats_features_first)):\n","  matrix_lagged = lag_feature(matrix, Stats_lags, Stats_features_first[i])\n","  matrix = pd.merge(matrix, matrix_lagged, on=['date_block_num','shop_id','item_id'], how='left')\n","del matrix_lagged\n","print(Stats_features_first)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qNf5zht40R6f","executionInfo":{"status":"aborted","timestamp":1602333273262,"user_tz":-60,"elapsed":153819,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["for i in range(len(Stats_features_second)):\n","  matrix_lagged = lag_feature(matrix, Stats_lags, Stats_features_second[i])\n","  matrix = pd.merge(matrix, matrix_lagged, on=['date_block_num','shop_id','item_id'], how='left')\n","del matrix_lagged\n","print(Stats_features_second)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F8adt0-u0UF0","executionInfo":{"status":"aborted","timestamp":1602333273263,"user_tz":-60,"elapsed":153812,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["for i in range(len(Stats_features_third)):\n","  matrix_lagged = lag_feature(matrix, Stats_lags, Stats_features_third[i])\n","  matrix = pd.merge(matrix, matrix_lagged, on=['date_block_num','shop_id','item_id'], how='left')\n","del matrix_lagged\n","print(Stats_features_third)\n","fetures_to_drop = TS_features + Stats_features #features are renamed and added as a new column within the lag_features functions, so remove these one\n","matrix = matrix.drop(fetures_to_drop, axis = 1)\n","matrix = matrix.fillna(0)\n","matrix[matrix['date_block_num']==13].head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hgmFHyzKX0EM"},"source":["2nd Stage: Trend based features\n","\n","\n","> Rate of change of Time series based features (mean of price or item count at past lags/months). Rates of change are calclulated for the past 1m\n","\n"]},{"cell_type":"code","metadata":{"id":"MvqHzOWVWbwv","executionInfo":{"status":"aborted","timestamp":1602333273264,"user_tz":-60,"elapsed":153805,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["ts = time.time()\n","\n","for TS_feature in TS_features:\n","  for i in trend_lags:\n","    matrix['trend_' + TS_feature + '_lag_'+str(i-1)] = \\\n","        (matrix[TS_feature +'_lag_'+str(i)] - matrix[TS_feature + '_lag_1']) / matrix[TS_feature + '_lag_1']\n","print(time.time()-ts)\n","matrix.tail()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wQOL4w4XhqSB","executionInfo":{"status":"aborted","timestamp":1602333273264,"user_tz":-60,"elapsed":153796,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["import numpy as np\n","matrix = matrix.replace([np.inf, -np.inf], np.nan)\n","matrix.fillna(0, inplace=True)\n","matrix[matrix['date_block_num'] ==  14].head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i7KKi2LH7ZVU"},"source":["# Load/Save Preprocessed Data"]},{"cell_type":"code","metadata":{"id":"Xcvd5MI7_B9x","executionInfo":{"status":"aborted","timestamp":1602333273265,"user_tz":-60,"elapsed":153793,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["file_dir_path_not_in_git = '/content/drive/My Drive/Colab Notebooks/Datasets/Kaggle_Coursera/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0L-hTqRPyEJn"},"source":["Write"]},{"cell_type":"code","metadata":{"id":"E8GbWiS6Ms4f","executionInfo":{"status":"aborted","timestamp":1602333273266,"user_tz":-60,"elapsed":153785,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["#only use if intending to write data\n","filename_to_write = 'TS&Stats_features_Submission'\n","data = matrix[matrix['date_block_num'] >= 14]\n","data.reset_index().astype('float32').to_feather(file_dir_path_not_in_git + filename_to_write + '.feather')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r0gOK6r3yFVV"},"source":["Read"]},{"cell_type":"code","metadata":{"id":"swBPhtAcM5gX","executionInfo":{"status":"aborted","timestamp":1602333273266,"user_tz":-60,"elapsed":153773,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["filename_to_read = 'TS&Stats_features_Submission'\n","matrix = pd.read_feather(file_dir_path_not_in_git + filename_to_read + '.feather', columns=None, use_threads=True)\n","matrix = matrix.astype({'index': np.int32, 'shop_id':np.int32,'item_id':np.int32, 'date_block_num':np.int32, 'ID':np.int32}).set_index('index')\n","matrix = matrix.replace([np.inf, -np.inf], np.nan)\n","matrix.fillna(0, inplace=True)\n","matrix.tail()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"24nOa-xbAu2W"},"source":["Merge read features tables with any created features"]},{"cell_type":"code","metadata":{"id":"5N3MgvnyA6k2","executionInfo":{"status":"aborted","timestamp":1602333273267,"user_tz":-60,"elapsed":153766,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["matrix = pd.merge(matrix, days_by_month.rename(columns= {'month': 'date_block_num'}), on=['date_block_num'], how='left')\n","matrix.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"58Mi48kIETHR","executionInfo":{"status":"aborted","timestamp":1602333273268,"user_tz":-60,"elapsed":153762,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["#Bad mean encoding\n","moy_mean = matrix.groupby(['MoY']).agg({'item_cnt_month':'mean'})\n","moy_mean.columns = ['mean_sales_by_MoY']\n","moy_mean.reset_index(inplace=True)\n","matrix = pd.merge(matrix,moy_mean , on=['MoY'], how='left')\n","matrix.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jtZ0iBdUCD-Z"},"source":["# Modelling\n","\n","\n","\n","*   Train/Val/Test split\n","*   Model Fit & Validate\n","*   Test/Submission Results\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9TJk9bzeCjqF"},"source":["**Train/Test split**"]},{"cell_type":"code","metadata":{"id":"l_RFPPPP9yr6","executionInfo":{"status":"aborted","timestamp":1602333273269,"user_tz":-60,"elapsed":153758,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["data = matrix\n","\n","use_toy_data = False #to be used just for code to run quicker when tests are needed to be made\n","if use_toy_data == True:\n","  train_start_index = 28\n","else:\n","  train_start_index = 14 #skip first 13 months - used to caclulate time series features\n","train_final_index = 28 #set to 28 usually: makes validation set to be 20% of the non-test data (threshold is surely debatable)\n","\n","data = data[data['date_block_num'] >= train_start_index]\n","\n","X_train = data[data.date_block_num <= train_final_index].drop(['item_cnt_month', 'ID'], axis=1)\n","y_train = data[data.date_block_num <= train_final_index]['item_cnt_month'].values\n","X_val = data[(data.date_block_num > train_final_index) & (data.date_block_num <= 33)].drop(['item_cnt_month', 'ID'], axis=1)\n","y_val = data[(data.date_block_num > train_final_index) & (data.date_block_num <= 33)]['item_cnt_month'].values\n","X_test = data[data.date_block_num == 34].drop(['item_cnt_month', 'ID'], axis=1)\n","try:\n","  X_test = pd.merge(test, X_test, on= ['date_block_num', 'item_id', 'shop_id']).drop(['ID'], axis = 1) #to ensure consistency in rows with test sumbission file\n","except:\n","  X_test = pd.merge(test, X_test, on= ['item_id', 'shop_id']).drop(['ID'], axis = 1) #to ensure consistency in rows with test sumbission file\n","del data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3K2LrsbsKNQL"},"source":["Features Normalised"]},{"cell_type":"code","metadata":{"id":"SdypQTpLKJEa","executionInfo":{"status":"aborted","timestamp":1602333273269,"user_tz":-60,"elapsed":153740,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","LinRegFeaturesToDrop= ['date_block_num', 'shop_id', 'item_id', 'item_category_id']\n","scaler =  MinMaxScaler(feature_range=[-1,1])\n","\n","X_train_LinReg = scaler.fit_transform(X_train.drop(LinRegFeaturesToDrop, axis = 1))\n","X_val_LinReg = scaler.transform(X_val.drop(LinRegFeaturesToDrop, axis = 1))\n","X_test_LinReg = scaler.transform(X_test.drop(LinRegFeaturesToDrop, axis = 1))\n","feature_names_LinReg = X_train.drop(LinRegFeaturesToDrop, axis = 1).columns"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fKgy82IKKiWf"},"source":["Features in numpy"]},{"cell_type":"code","metadata":{"id":"57EJdoQsKhws","executionInfo":{"status":"aborted","timestamp":1602333273270,"user_tz":-60,"elapsed":153721,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["feature_names = X_train.columns\n","X_train_np = X_train.to_numpy(dtype = np.float32)\n","del X_train\n","X_val_np = X_val.to_numpy(dtype = np.float32)\n","del X_val\n","X_test_np = X_test.to_numpy(dtype = np.float32)\n","del X_test\n","X_train_np.nbytes/(10**6)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"izqoiZEgCoy-"},"source":["**Model Fit & Validate**"]},{"cell_type":"markdown","metadata":{"id":"HprRItOZV8o6"},"source":["LightGBM"]},{"cell_type":"code","metadata":{"id":"t-Mfy5-NNv98","executionInfo":{"status":"aborted","timestamp":1602333273271,"user_tz":-60,"elapsed":153716,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["load_model = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FdwdzVxXVmqd","executionInfo":{"status":"aborted","timestamp":1602333273272,"user_tz":-60,"elapsed":153698,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["model_name = '/LGBM_submission_model' \n","X_train_model, X_val_model, X_test_model = X_train_np, X_val_np, X_test_np #X_train_np, X_val_np, X_test_np\n","y_train_model = y_train\n","\n","start = time.time()\n","if load_model == True:\n","  model = pickle.load(open(file_dir_path + model_name + '.sav', 'rb'))\n","else:\n","  model = LGBMRegressor(n_estimators=50, max_depth = 8, importance_type = 'gain')\n","  model.fit(X_train_model, y_train_model, eval_set=[(X_val_model, y_val), (X_train_model, y_train_model)], verbose=0)\n","\n","y_pred_train, y_pred_val, y_pred_test =  model.predict(X_train_model).clip(0,20), model.predict(X_val_model).clip(0,20), model.predict(X_test_model).clip(0,20)\n","train_score, val_score = sklearn.metrics.r2_score(y_train_model, y_pred_train), sklearn.metrics.r2_score(y_val, y_pred_val)\n","train_score, val_score = sklearn.metrics.r2_score(y_train_model, y_pred_train), sklearn.metrics.r2_score(y_val, y_pred_val)\n","train_rmse, val_rmse = np.sqrt(sklearn.metrics.mean_squared_error(y_train_model, y_pred_train)), np.sqrt(sklearn.metrics.mean_squared_error(y_val, y_pred_val))\n","print('R^2 train_score is ' + str(train_score) + ' R^2 val_score is ' + str(val_score))\n","print('RMSE train_score is ' + str(train_rmse) + ' RMSE val_score is ' + str(val_rmse))\n","print(time.time()-start)\n","\n","lgb.plot_metric(model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ymU5jr8zf1eF"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.svm import SVC\n","from sklearn.datasets import load_digits\n","from sklearn.model_selection import learning_curve\n","from sklearn.model_selection import ShuffleSplit\n","\n","\n","def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n","                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n","    \"\"\"\n","    Generate 3 plots: the test and training learning curve, the training\n","    samples vs fit times curve, the fit times vs score curve.\n","\n","    Parameters\n","    ----------\n","    estimator : object type that implements the \"fit\" and \"predict\" methods\n","        An object of that type which is cloned for each validation.\n","\n","    title : string\n","        Title for the chart.\n","\n","    X : array-like, shape (n_samples, n_features)\n","        Training vector, where n_samples is the number of samples and\n","        n_features is the number of features.\n","\n","    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n","        Target relative to X for classification or regression;\n","        None for unsupervised learning.\n","\n","    axes : array of 3 axes, optional (default=None)\n","        Axes to use for plotting the curves.\n","\n","    ylim : tuple, shape (ymin, ymax), optional\n","        Defines minimum and maximum yvalues plotted.\n","\n","    cv : int, cross-validation generator or an iterable, optional\n","        Determines the cross-validation splitting strategy.\n","        Possible inputs for cv are:\n","\n","          - None, to use the default 5-fold cross-validation,\n","          - integer, to specify the number of folds.\n","          - :term:`CV splitter`,\n","          - An iterable yielding (train, test) splits as arrays of indices.\n","\n","        For integer/None inputs, if ``y`` is binary or multiclass,\n","        :class:`StratifiedKFold` used. If the estimator is not a classifier\n","        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n","\n","        Refer :ref:`User Guide <cross_validation>` for the various\n","        cross-validators that can be used here.\n","\n","    n_jobs : int or None, optional (default=None)\n","        Number of jobs to run in parallel.\n","        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n","        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n","        for more details.\n","\n","    train_sizes : array-like, shape (n_ticks,), dtype float or int\n","        Relative or absolute numbers of training examples that will be used to\n","        generate the learning curve. If the dtype is float, it is regarded as a\n","        fraction of the maximum size of the training set (that is determined\n","        by the selected validation method), i.e. it has to be within (0, 1].\n","        Otherwise it is interpreted as absolute sizes of the training sets.\n","        Note that for classification the number of samples usually have to\n","        be big enough to contain at least one sample from each class.\n","        (default: np.linspace(0.1, 1.0, 5))\n","    \"\"\"\n","    if axes is None:\n","        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n","\n","    axes[0].set_title(title)\n","    if ylim is not None:\n","        axes[0].set_ylim(*ylim)\n","    axes[0].set_xlabel(\"Training examples\")\n","    axes[0].set_ylabel(\"Score\")\n","\n","    train_sizes, train_scores, test_scores, fit_times, _ = \\\n","        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n","                       train_sizes=train_sizes,\n","                       return_times=True)\n","    train_scores_mean = np.mean(train_scores, axis=1)\n","    train_scores_std = np.std(train_scores, axis=1)\n","    test_scores_mean = np.mean(test_scores, axis=1)\n","    test_scores_std = np.std(test_scores, axis=1)\n","    fit_times_mean = np.mean(fit_times, axis=1)\n","    fit_times_std = np.std(fit_times, axis=1)\n","\n","    # Plot learning curve\n","    axes[0].grid()\n","    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n","                         train_scores_mean + train_scores_std, alpha=0.1,\n","                         color=\"r\")\n","    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n","                         test_scores_mean + test_scores_std, alpha=0.1,\n","                         color=\"g\")\n","    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n","                 label=\"Training score\")\n","    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n","                 label=\"Cross-validation score\")\n","    axes[0].legend(loc=\"best\")\n","\n","    # Plot n_samples vs fit_times\n","    axes[1].grid()\n","    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n","    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n","                         fit_times_mean + fit_times_std, alpha=0.1)\n","    axes[1].set_xlabel(\"Training examples\")\n","    axes[1].set_ylabel(\"fit_times\")\n","    axes[1].set_title(\"Scalability of the model\")\n","\n","    # Plot fit_time vs score\n","    axes[2].grid()\n","    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n","    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n","                         test_scores_mean + test_scores_std, alpha=0.1)\n","    axes[2].set_xlabel(\"fit_times\")\n","    axes[2].set_ylabel(\"Score\")\n","    axes[2].set_title(\"Performance of the model\")\n","\n","    return plt\n","\n","\n","fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n","\n","\n","title = \"Learning Curves (Naive Bayes)\"\n","# Cross validation with 100 iterations to get smoother mean test and train\n","# score curves, each time with 20% data randomly selected as a validation set.\n","cv = X\n","\n","estimator = GaussianNB()\n","plot_learning_curve(estimator, title, X, y, axes=axes[:, 0], ylim=(0.7, 1.01),\n","                    cv=cv, n_jobs=4)\n","\n","title = r\"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\n","# SVC is more expensive so we do a lower number of CV iterations:\n","cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n","estimator = SVC(gamma=0.001)\n","plot_learning_curve(estimator, title, X, y, axes=axes[:, 1], ylim=(0.7, 1.01),\n","                    cv=cv, n_jobs=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XXljey1q89L0","executionInfo":{"status":"aborted","timestamp":1602333273272,"user_tz":-60,"elapsed":153677,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["import matplotlib.pyplot as plt\n","# Plot feature importance - Results Visualization\n","feature_importance = model.feature_importances_\n","# make importances relative to max importance\n","feature_importance = 100.0 * (feature_importance / feature_importance.max())\n","sorted_idx = np.argsort(feature_importance)\n","pos = np.arange(sorted_idx.shape[0]) + .5\n","plt.figure(figsize=(13,30)) \n","plt.barh(pos, feature_importance[sorted_idx], align='center')\n","plt.yticks(pos, feature_names[sorted_idx])\n","plt.xlabel('Relative Importance')\n","plt.title('Variable Importance')\n","plt.tick_params(axis='y', which='major', labelsize = 13)\n","plt.show()\n","#plt.savefig('gbt_feature_importance.png')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t0BYhtbBW52M","executionInfo":{"status":"aborted","timestamp":1602333273274,"user_tz":-60,"elapsed":153661,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["np.mean(y_pred_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pnChGfvV9FgZ"},"source":["Submission prep"]},{"cell_type":"code","metadata":{"id":"aG-2lGlPEBWN","executionInfo":{"status":"aborted","timestamp":1602333273275,"user_tz":-60,"elapsed":153645,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["submission = pd.DataFrame({\n","    \"ID\": test.index, \n","    \"item_cnt_month\": y_pred_test\n","})\n","submission.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6KHbQ6zpED7v"},"source":["Save submission"]},{"cell_type":"code","metadata":{"id":"P8yW1V029E4Z","executionInfo":{"status":"aborted","timestamp":1602333273276,"user_tz":-60,"elapsed":153641,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["#save submission file\n","submission.to_csv(file_dir_path + model_name + '_submission.csv', index=False)\n","\n","# save the model to disk\n","pickle.dump(model, open(file_dir_path + model_name + '.sav', 'wb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-NHEVfQFPS9S","executionInfo":{"status":"aborted","timestamp":1602333273276,"user_tz":-60,"elapsed":153625,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["import os\n","os.getcwd()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UhzCtOrUPfSQ","executionInfo":{"status":"aborted","timestamp":1602333273277,"user_tz":-60,"elapsed":153620,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["submission.to_csv('testing_submission.csv', index=False)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZOYpO6i6IOIo"},"source":["NN"]},{"cell_type":"code","metadata":{"id":"IUXNzrsDIOhE","executionInfo":{"status":"aborted","timestamp":1602333273278,"user_tz":-60,"elapsed":153606,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.optimizers import Adam\n","from keras.metrics import RootMeanSquaredError\n","\n","X_train_model, X_val_model, X_test_model = X_train_LinReg, X_val_LinReg, X_test_LinReg\n","y_train_model = y_train\n","'''\n","from sklearn.decomposition import PCA\n","# Make an instance of the Model\n","pca = PCA(.95)\n","pca.fit(X_train_model)\n","X_train_model, X_val_model, X_test_model = pca.transform(X_train_model), pca.transform(X_val_model), pca.transform(X_test_model)\n","'''\n","\n","#subsample\n","'''\n","sampling_perc = 0.1 #percentage to sample out of total population of data points. 10**-4 takes 18 secs\n","random_indices = np.random.choice(len(X_train_model), int(len(X_train_model)*sampling_perc), replace = False)\n","X_train_model, y_train_model = X_train_model[random_indices], y_train[random_indices]\n","'''\n","\n","#Train\n","model = Sequential()\n","model.add(Dense(20, input_dim=X_train_model.shape[1], activation='linear'))\n","model.add(Dense(20, activation='linear'))\n","model.add(Dense(1, activation='linear'))\n","model.compile(loss='mse', optimizer=Adam(learning_rate=0.01), metrics=[RootMeanSquaredError(name='rmse')])\n","model_history = model.fit(X_train_model, y_train_model, epochs=2, batch_size=3200, validation_data=(X_val_model, y_val), shuffle = True)\n","\n","#Predict\n","y_pred_train, y_pred_val, y_pred_test =  model.predict(X_train_model).clip(0,20), model.predict(X_val_model).clip(0,20), model.predict(X_test_model).clip(0,20)\n","train_score, val_score = sklearn.metrics.r2_score(y_train_model, y_pred_train), sklearn.metrics.r2_score(y_val, y_pred_val)\n","train_rmse, val_rmse = np.sqrt(sklearn.metrics.mean_squared_error(y_train_model, y_pred_train)), np.sqrt(sklearn.metrics.mean_squared_error(y_val, y_pred_val))\n","print('R^2 train_score is ' + str(train_score) + ' R^2 val_score is ' + str(val_score))\n","print('RMSE train_score is ' + str(train_rmse) + ' RMSE val_score is ' + str(val_rmse))\n","plotKerasLearningCurves(model_history)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9raFxaxKJ2Jq","executionInfo":{"status":"aborted","timestamp":1602333273279,"user_tz":-60,"elapsed":153601,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":["def plotKerasLearningCurves(history):\n","  plt.plot(history.history['loss'])\n","  plt.plot(history.history['val_loss'])\n","  plt.title('model loss')\n","  plt.ylabel('score')\n","  plt.xlabel('epoch')\n","  plt.legend(['train', 'test'], loc='upper left')\n","  plt.show()\n","  # summarize history for loss\n","  plt.plot(history.history['loss'])\n","  plt.plot(history.history['val_loss'])\n","  plt.title('model loss')\n","  plt.ylabel('loss')\n","  plt.xlabel('epoch')\n","  plt.legend(['train', 'test'], loc='upper left')\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DAQlRfxKR7M7","executionInfo":{"status":"aborted","timestamp":1602333273280,"user_tz":-60,"elapsed":153597,"user":{"displayName":"Andreas Theodoulou","photoUrl":"","userId":"10256955413424659339"}}},"source":[""],"execution_count":null,"outputs":[]}]}